# Databricks notebook source
skip = "Yes" if "config" in locals() else ""

# COMMAND ----------

# MAGIC %run ../02/config/quiet $skip=skip

# COMMAND ----------

unpack_config(config)

# COMMAND ----------

# MAGIC %run ../common/utils/TABLE_NAMES

# COMMAND ----------

try:
  test
except:
  test = True

# COMMAND ----------

spark.sql("set spark.sql.legacy.timeParserPolicy=LEGACY")

# COMMAND ----------

spark.sql(f"""
CREATE OR REPLACE TEMP VIEW {preamble}_deaths_to_join AS
SELECT
  1 AS DEATH,
  MAX(NEO_NATE_FLAG) AS NEO_NATE_FLAG,
  DEC_CONF_NHS_NUMBER_CLEAN_DEID AS PERSON_ID_DEID,
  MIN(to_date(REG_DATE_OF_DEATH, 'yyyyMMdd')) AS REG_DATE_OF_DEATH,
  CONCAT_WS(',', COLLECT_LIST(to_date(REG_DATE_OF_DEATH, 'yyyyMMdd'))) AS ALL_REG_DATE_OF_DEATH_debug,
  COUNT(*) AS RECORD_COUNT,
  NULLIF(CONCAT_WS(',', FLATTEN(COLLECT_LIST(SPLIT(NULLIF(CONCAT_WS(',', S_COD_CODE_1, S_COD_CODE_2, S_COD_CODE_3, S_COD_CODE_4, S_COD_CODE_5, S_COD_CODE_6, S_COD_CODE_7, S_COD_CODE_8, S_COD_CODE_9, S_COD_CODE_10, S_COD_CODE_11, S_COD_CODE_12, S_COD_CODE_13, S_COD_CODE_14, S_COD_CODE_15), ''), ',')))), '') AS COD_CODES,
  NULLIF(CONCAT_WS(',', ARRAY_DISTINCT(FLATTEN(COLLECT_LIST(SPLIT(NULLIF(CONCAT_WS(',', S_COD_CODE_1, S_COD_CODE_2, S_COD_CODE_3, S_COD_CODE_4, S_COD_CODE_5, S_COD_CODE_6, S_COD_CODE_7, S_COD_CODE_8, S_COD_CODE_9, S_COD_CODE_10, S_COD_CODE_11, S_COD_CODE_12, S_COD_CODE_13, S_COD_CODE_14, S_COD_CODE_15), ''), ','))))), '') AS UNIQUE_COD_CODES,
  NULLIF(CONCAT_WS(',', FLATTEN(COLLECT_LIST(SPLIT(NULLIF(CONCAT_WS(',', ARRAY_EXCEPT(ARRAY(S_COD_CODE_1, S_COD_CODE_2, S_COD_CODE_3, S_COD_CODE_4, S_COD_CODE_5, S_COD_CODE_6, S_COD_CODE_7, S_COD_CODE_8, S_COD_CODE_9, S_COD_CODE_10, S_COD_CODE_11, S_COD_CODE_12, S_COD_CODE_13, S_COD_CODE_14, S_COD_CODE_15), ARRAY(S_UNDERLYING_COD_ICD10))), ''), ',')))), '') AS COD_CODES_SECONDARY,
  NULLIF(CONCAT_WS(',', ARRAY_DISTINCT(FLATTEN(COLLECT_LIST(SPLIT(NULLIF(CONCAT_WS(',', ARRAY_EXCEPT(ARRAY(S_COD_CODE_1, S_COD_CODE_2, S_COD_CODE_3, S_COD_CODE_4, S_COD_CODE_5, S_COD_CODE_6, S_COD_CODE_7, S_COD_CODE_8, S_COD_CODE_9, S_COD_CODE_10, S_COD_CODE_11, S_COD_CODE_12, S_COD_CODE_13, S_COD_CODE_14, S_COD_CODE_15), ARRAY(S_UNDERLYING_COD_ICD10))), ''), ','))))), '') AS UNIQUE_COD_CODES_SECONDARY,
  NULLIF(CONCAT_WS(',', COLLECT_LIST(NULLIF(S_UNDERLYING_COD_ICD10, ''))), '') AS COD_CODES_PRIMARY,
  NULLIF(CONCAT_WS(',', ARRAY_DISTINCT(COLLECT_LIST(NULLIF(S_UNDERLYING_COD_ICD10, '')))), '') AS UNIQUE_COD_CODES_PRIMARY
FROM {collab_database}.{preamble}_deaths
WHERE
  DEC_CONF_NHS_NUMBER_CLEAN_DEID IS NOT NULL
  AND to_date(REG_DATE_OF_DEATH, 'yyyyMMdd') BETWEEN '{study_start}' AND '{study_end}'
GROUP BY PERSON_ID_DEID""")

# COMMAND ----------

spark.sql(f"""
CREATE OR REPLACE TEMP VIEW {preamble}_deaths_cohort AS
SELECT
  GREATEST(0, DATEDIFF(REG_DATE_OF_DEATH, DOB) / 365.25) AS AGE,
  DOB,
  {generate_variant_period_case_when('REG_DATE_OF_DEATH', variant_period_dict)} AS DOD_VARIANT_PERIOD,
  NEO_NATE_FLAG,
  REG_DATE_OF_DEATH,
  ALL_REG_DATE_OF_DEATH_debug,
  RECORD_COUNT,
  CASE WHEN COD_CODES_PRIMARY RLIKE '{primary_codes}' THEN 1 ELSE 0 END AS COVID_PRIMARY_COD, 
  CASE WHEN COD_CODES_SECONDARY RLIKE 'U071|U072' THEN 1 ELSE 0 END AS COVID_SECONDARY_COD,
  CASE WHEN (REG_DATE_OF_DEATH BETWEEN '{pims_defn_date}' AND '2021-12-13' AND (COD_CODES_PRIMARY RLIKE 'U075' OR (COD_CODES_PRIMARY RLIKE 'M303|R65' AND COD_CODES NOT RLIKE '{pims_reject_code_statement}'))) OR (REG_DATE_OF_DEATH > '2021-12-13' AND COD_CODES_PRIMARY RLIKE 'U075') THEN 1 ELSE 0 END AS PIMS_PRIMARY_COD,
  CASE WHEN (REG_DATE_OF_DEATH >= '{pims_defn_date}' AND (COD_CODES_SECONDARY RLIKE 'U075' OR (COD_CODES_SECONDARY RLIKE 'M303|R65' AND COD_CODES NOT RLIKE '{pims_reject_code_statement}'))) OR (REG_DATE_OF_DEATH > '2021-12-13' AND COD_CODES_SECONDARY RLIKE 'U075') THEN 1 ELSE 0 END AS PIMS_SECONDARY_COD,
  COD_CODES,
  UNIQUE_COD_CODES,
  COD_CODES_SECONDARY,
  UNIQUE_COD_CODES_SECONDARY,
  COD_CODES_PRIMARY,
  UNIQUE_COD_CODES_PRIMARY
FROM {preamble}_deaths_to_join a
INNER JOIN {collab_database}.{preamble}_skinny b
ON a.PERSON_ID_DEID = b.PERSON_ID_DEID
WHERE (DOB IS NOT NULL AND GREATEST(0, DATEDIFF(REG_DATE_OF_DEATH, DOB) / 365.25) < 18) OR NEO_NATE_FLAG == 1""")

# COMMAND ----------

display(spark.sql(f"""
SELECT
    DOD_VARIANT_PERIOD,
    COUNT(*) AS TOTAL_UNDER_18_DEATHS,
    SUM(NEO_NATE_FLAG) AS TOTAL_NEO_NATE_DEATHS,
    SUM(CASE WHEN NEO_NATE_FLAG > 0 AND COVID_PRIMARY_COD + COVID_SECONDARY_COD + PIMS_PRIMARY_COD + PIMS_SECONDARY_COD > 0 THEN 1 ELSE 0 END) AS NEO_NATE_COVID_OR_PIMS,
    SUM(CASE WHEN COVID_PRIMARY_COD + PIMS_PRIMARY_COD + COVID_SECONDARY_COD + PIMS_SECONDARY_COD > 0 THEN 1 ELSE 0 END) AS COVID_OR_PIMS_RELATED_COD,
    SUM(CASE WHEN COVID_PRIMARY_COD + PIMS_PRIMARY_COD > 0 THEN 1 ELSE 0 END) AS COVID_OR_PIMS_UNDERLYING_COD,
    SUM(COVID_SECONDARY_COD) AS COVID_SECONDARY_COD,
    SUM(PIMS_PRIMARY_COD) AS PIMS_PRIMARY_COD,
    SUM(PIMS_SECONDARY_COD) AS PIMS_SECONDARY_COD
FROM {preamble}_deaths_cohort
GROUP BY DOD_VARIANT_PERIOD
ORDER BY DOD_VARIANT_PERIOD"""))

# COMMAND ----------


